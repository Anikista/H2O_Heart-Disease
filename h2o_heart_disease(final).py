# -*- coding: utf-8 -*-
"""H2O_heart_disease(Final).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wx_jw-QrJvppcmmMCqz3eD43_ciw6V0u
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

from imblearn.over_sampling import RandomOverSampler

from sklearn.gaussian_process.kernels import RBF
import time
kernel = 1.0 * RBF(1.0)

import tensorflow as tf
import keras
#import keras_metrics
from keras.utils import to_categorical
from keras.models import Sequential
from keras.optimizers import SGD
from keras.layers import Dense
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import RocCurveDisplay
from sklearn.inspection import PartialDependenceDisplay
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score
import time

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import auc
from sklearn.metrics import roc_curve

from sklearn import metrics

tf.random.set_seed(123)

!pip install h2o
import h2o
h2o.init()
from h2o.model.segment_models import H2OFrame
from h2o.automl import H2OAutoML
print("All Library Loaded")

df  = pd.read_csv("/content/drive/MyDrive/heart.csv")

for i in range(len(df.columns)):
  if i != 0 and i != 3 and i != 4 and i != 7 and i != 9:
    df.iloc[:,i] = df.iloc[:,i].astype('category')
  
df_td = df.copy()
X = df_td.iloc[:,0:-1].copy()
Y = df_td['target'].copy()
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)

#df.head()

#df  = pd.read_csv("/content/drive/MyDrive/heart.csv")
df.corr()





df["target"].value_counts()

526+499





#KNN, SVM, LR, GP

#X = df_td.iloc[:,0:-1].copy()
#Y = df_td['target'].copy()
#X = X.values

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)

"""
estimators = list(range(1, 20))
accuracytest = []
accuracytrain = []

for n_estimators in estimators:
    clf = KNeighborsClassifier(n_neighbors=n_estimators,p=2,n_jobs=4,
                               weights='uniform').fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    accuracytest.append(acc)
    acc = clf.score(X_train, y_train)
    accuracytrain.append(acc)

plt.plot(estimators, accuracytrain, label="train_accuracy")
plt.plot(estimators, accuracytest, label="test_accuracy")
plt.plot(estimators, (pd.DataFrame(accuracytrain) - pd.DataFrame(accuracytest)), label="difference_between_train_and_test_accuracy")
plt.plot(estimators, np.zeros(len(estimators)), label="zero_line")
plt.title("KNeighborsClassifier")
plt.xlabel("Number of neighbours")
plt.ylabel("Accuracy")
plt.legend(loc='best')
plt.show()
"""

neigh = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)
NTr = neigh.score(X_train, y_train)
NTe = neigh.score(X_test, y_test)
print("train accuracy: ",NTr)
print("test accuracy: ",NTe)

estimators = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]
#estimators = ['poly', 'rbf', 'sigmoid']
accuracytest = []
accuracytrain = []

for n_estimators in estimators:
    clf = SVC(kernel = "poly",degree=n_estimators,
              random_state=123,
              probability=True).fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    accuracytest.append(acc)
    acc = clf.score(X_train, y_train)
    accuracytrain.append(acc)

plt.plot(estimators, accuracytrain, label="train_accuracy")
plt.plot(estimators, accuracytest, label="test_accuracy")
plt.plot(estimators, (pd.DataFrame(accuracytrain) - pd.DataFrame(accuracytest)), label="difference_between_train_and_test_accuracy")
plt.plot(estimators, np.zeros(len(estimators)), label="zero_line")
plt.title("SVC(kernel=poly)")
plt.xlabel("Number of Degree")
plt.ylabel("Accuracy")
plt.legend(loc='best')
plt.show()

"""estimators = [5,10,15,20,25,30,35,40,45]
#estimators = ['poly', 'rbf', 'sigmoid']
accuracytest = []
accuracytrain = []

for n_estimators in estimators:
    clf = SVC(C = n_estimators ,kernel = "poly",degree=10,
              random_state=123,
              probability=True).fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    accuracytest.append(acc)
    acc = clf.score(X_train, y_train)
    accuracytrain.append(acc)

plt.plot(estimators, accuracytrain, label="train_accuracy")
plt.plot(estimators, accuracytest, label="test_accuracy")
plt.plot(estimators, (pd.DataFrame(accuracytrain) - pd.DataFrame(accuracytest)), label="difference_between_train_and_test_accuracy")
plt.plot(estimators, np.zeros(len(estimators)), label="zero_line")
plt.title("SVC(kernel=poly, degree = 10)")
plt.xlabel("Number of C")
plt.ylabel("Accuracy")
plt.legend(loc='best')
plt.show()

#estimators = [5,10,15,20,25,30,35,40,45]
estimators = ['poly', 'rbf', 'sigmoid']
accuracytest = []
accuracytrain = []

for n_estimators in estimators:
    clf = SVC(C = 40 ,kernel = n_estimators,degree=10,
              random_state=123,
              probability=True).fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    accuracytest.append(acc)
    acc = clf.score(X_train, y_train)
    accuracytrain.append(acc)

plt.plot(estimators, accuracytrain, label="train_accuracy")
plt.plot(estimators, accuracytest, label="test_accuracy")
plt.plot(estimators, (pd.DataFrame(accuracytrain) - pd.DataFrame(accuracytest)), label="difference_between_train_and_test_accuracy")
plt.plot(estimators, np.zeros(len(estimators)), label="zero_line")
plt.title("SVC(C=40kernel=poly, degree = 10)")
plt.xlabel("kernels")
plt.ylabel("Accuracy")
plt.legend(loc='best')
plt.show()
"""

svc = SVC(C = 40, kernel = "poly",degree = 10,
          random_state=123, probability=True).fit(X_train, y_train)
STr = svc.score(X_train, y_train)
STe = svc.score(X_test, y_test)
print("train accuracy: ",STr)
print("test accuracy: ",STe)

LR = LogisticRegression(penalty="l2",
                        max_iter=1000,random_state=123).fit(X_train, y_train)
LTr = LR.score(X_train, y_train)
LTe = LR.score(X_test, y_test)
print("train accuracy: ",LTr)
print("test accuracy: ",LTe)

LR

#DT, ANN, Ensemble
from sklearn import tree

"""
DT = DecisionTreeClassifier(random_state=123).fit(X_train, y_train)
#DT = DecisionTreeClassifier(random_state=123,max_depth=4,max_features='sqrt',splitter = "random",ccp_alpha = 0.01).fit(X_train, y_train)
DTr = DT.score(X_train, y_train)
DTe = DT.score(X_test, y_test)
print("train accuracy: ",DTr)
print("test accuracy: ",DTe)
path = DT.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
ccp_alphas = (ccp_alphas.round(5))
impurities = (impurities.round(5))
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1],impurities[:-1],marker = 'o',drawstyle = "steps-post")
plt.title("DecisionTreeClassifier(default_hyperparameters)")
ax.set_xlabel("alpha")
ax.set_ylabel("impurities")
plt.show()
"""

#tree.plot_tree(DT)

#DT = DecisionTreeClassifier(random_state=123).fit(X_train, y_train)
DT = DecisionTreeClassifier(random_state=123,max_depth=5,
                            max_features='sqrt',
                            splitter = "random",
                            ccp_alpha = 0.02).fit(X_train, y_train)
DTr = DT.score(X_train, y_train)
DTe = DT.score(X_test, y_test)
print("train accuracy: ",DTr)
print("test accuracy: ",DTe)
"""
path = DT.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
ccp_alphas = (ccp_alphas.round(5))
impurities = (impurities.round(5))
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1],impurities[:-1],marker = 'o',drawstyle = "steps-post")
plt.title("DecisionTreeClassifier(ccp_alpha = 0.02)")
ax.set_xlabel("alpha")
ax.set_ylabel("impurities")
plt.show()
"""

#tree.plot_tree(DT)

#df_td.iloc[:,0:-1]

Tac = []
#Tpr = []
#Tre = []
teac = []
tepr = []
tere = []
sc = StandardScaler()
for i in range(1):
  X = df_td.iloc[:,0:-1].copy()
  X = sc.fit_transform(X)
  y = df_td['target'].copy()
  encoder = LabelEncoder()
  encoder.fit(y)
  y = encoder.transform(y)
  ##y = to_categorical(y)
  X_traink,X_testk,y_traink,y_testk = train_test_split(X, y ,test_size = 0.2, random_state=123)
  X_testkk,X_val,y_testkk,y_val = train_test_split(X_testk, y_testk ,test_size = 0.5, random_state=123)
  
  #X_traink = sc.fit_transform(X_traink)
  #X_testk  = sc.fit_transform(X_testk)
  #X_val  = sc.fit_transform(X_val)
  model = Sequential()
  model.add(Dense(13, activation='relu', input_shape=(X_traink.shape[1],)))
  model.add(Dense(13, activation='selu'))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(loss='BinaryCrossentropy', optimizer='Adam', metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
  start_time = time.time()
  model.fit(X_traink,y_traink, batch_size = 64, epochs = 20, validation_data = (X_val,y_val),)
  end_time = time.time()

  ATr = model.evaluate(X_traink,y_traink,verbose=0)#[1]
  ATe = model.evaluate(X_testkk,y_testkk,verbose=0)#[1]

  Tac.append(ATr[1])
  #Tpr.append(ATr[2])
  #Tre.append(ATr[3])

  teac.append(ATe[1])
  tepr.append(ATe[2])
  tere.append(ATe[3])



print(ATr,ATe)

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

import visualkeras
visualkeras.layered_view(model, legend=True,
                         draw_volume=True,spacing=30)

"""
estimators = list(range(1, 150))
accuracytest = []
accuracytrain = []

for n_estimators in estimators:
    clf = BaggingClassifier(DecisionTreeClassifier(random_state=123,
                            max_depth=4,max_features='sqrt',
                            splitter = "random",
                            ccp_alpha = 0.01),n_estimators=n_estimators, max_samples=150, max_features=3,random_state=123).fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    accuracytest.append(acc)
    acc = clf.score(X_train, y_train)
    accuracytrain.append(acc)

plt.plot(estimators, accuracytrain, label="train_accuracy")
plt.plot(estimators, accuracytest, label="test_accuracy")
plt.plot(estimators, (pd.DataFrame(accuracytrain) - pd.DataFrame(accuracytest)), label="difference_between_train_and_test_accuracy")
plt.plot(estimators, np.zeros(len(estimators)), label="zero_line")
plt.title("BaggingClassifierDecisionTreeClassifier")
plt.xlabel("Number of estimators")
plt.ylabel("Accuracy")
plt.legend(loc='best')
plt.show()
"""

BDT = BaggingClassifier(DecisionTreeClassifier(random_state=123,
                            max_depth=4,max_features='sqrt',
                            splitter = "random",
                            ccp_alpha = 0.01),n_estimators=100, max_samples=150, max_features=3,random_state=123).fit(X_train, y_train)
BDTr = BDT.score(X_train, y_train)
BDTe = BDT.score(X_test, y_test)
#tree.plot_tree(BDT)
print("train accuracy: ",BDTr)
print("test accuracy: ",BDTe)

40.49+44.88

"""
estimators = list(range(1, 150))
accuracytest = []
accuracytrain = []

for n_estimators in estimators:
    clf = BaggingClassifier(KNeighborsClassifier(n_neighbors=10),n_estimators=n_estimators, max_samples=150, max_features=3,random_state=123).fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    accuracytest.append(acc)
    acc = clf.score(X_train, y_train)
    accuracytrain.append(acc)

plt.plot(estimators, accuracytrain, label="train_accuracy")
plt.plot(estimators, accuracytest, label="test_accuracy")
plt.plot(estimators, (pd.DataFrame(accuracytrain) - pd.DataFrame(accuracytest)), label="difference_between_train_and_test_accuracy")
plt.plot(estimators, np.zeros(len(estimators)), label="zero_line")
plt.title("BaggingClassifierKNeighborsClassifier")
plt.xlabel("Number of estimators")
plt.ylabel("Accuracy")
plt.legend(loc='best')
plt.show()
"""

BKNN = BaggingClassifier(KNeighborsClassifier(n_neighbors=10),
                         n_estimators=16,max_samples=150,
                         max_features=3,
                         random_state=123).fit(X_train, y_train)
BKTr = BKNN.score(X_train, y_train)
BKTe = BKNN.score(X_test, y_test)
print("train accuracy: ",BKTr)
print("test accuracy: ",BKTe)

"""
estimators = list(range(1, 150))
accuracytest = []
accuracytrain = []

for n_estimators in estimators:
    clf = BaggingClassifier(GaussianProcessClassifier(),n_estimators=n_estimators, max_samples=150, max_features=3,random_state=123).fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    accuracytest.append(acc)
    acc = clf.score(X_train, y_train)
    accuracytrain.append(acc)

plt.plot(estimators, accuracytrain, label="train_accuracy")
plt.plot(estimators, accuracytest, label="test_accuracy")
plt.plot(estimators, (pd.DataFrame(accuracytrain) - pd.DataFrame(accuracytest)), label="difference_between_train_and_test_accuracy")
plt.plot(estimators, np.zeros(len(estimators)), label="zero_line")
plt.title("BaggingClassifierGaussianProcessClassifier")
plt.xlabel("Number of estimators")
plt.ylabel("Accuracy")
plt.legend(loc='best')
plt.show()
"""

BGPC = BaggingClassifier(GaussianProcessClassifier(),n_estimators=150, max_samples=150, max_features=3,random_state=123).fit(X_train, y_train)
BGTr = BGPC.score(X_train, y_train)
BGTe = BGPC.score(X_test, y_test)
print("train accuracy: ",BGTr)
print("test accuracy: ",BGTe)

"""
estimators = list(range(1, 150))
accuracytest = []
accuracytrain = []

for n_estimators in estimators:
    clf = BaggingClassifier(SVC(kernel = "poly",random_state=123),n_estimators=n_estimators, max_samples=150, max_features=3,random_state=123).fit(X_train, y_train)
    acc = clf.score(X_test, y_test)
    accuracytest.append(acc)
    acc = clf.score(X_train, y_train)
    accuracytrain.append(acc)

plt.plot(estimators, accuracytrain, label="train_accuracy")
plt.plot(estimators, accuracytest, label="test_accuracy")
plt.plot(estimators, (pd.DataFrame(accuracytrain) - pd.DataFrame(accuracytest)), label="difference_between_train_and_test_accuracy")
plt.plot(estimators, np.zeros(len(estimators)), label="zero_line")
plt.title("BaggingClassifierSVC")
plt.xlabel("Number of estimators")
plt.ylabel("Accuracy")
plt.legend(loc='best')
plt.show()
"""

BSVC = BaggingClassifier(SVC(kernel = "poly",random_state=123),
                         n_estimators=50, max_samples=150, 
                         max_features=3,
                         random_state=123).fit(X_train, y_train)
BSTr = BSVC.score(X_train, y_train)
BSTe = BSVC.score(X_test, y_test)
print("train accuracy: ",BSTr)
print("test accuracy: ",BSTe)

#df  = pd.read_csv("/content/drive/MyDrive/heart.csv")

#for i in range(len(df.columns)):
#  if i != 0 and i != 3 and i != 4 and i != 7 and i != 9:
#    df.iloc[:,i] = df.iloc[:,i].astype('category')
  
#df_td = df.copy()

#train, test= train_test_split(df_td, test_size=0.2, random_state=444)
#train, valid= train_test_split(test, test_size=0.5, random_state=444)

path = ["/content/drive/MyDrive/HeartDisease/GBM_3_AutoML_1_20230408_140006", 
        "/content/drive/MyDrive/HeartDisease/GBM_3_AutoML_3_20230408_171943"]
best_model = h2o.load_model(path[1])

df_h2o = df.copy()        
train, test = train_test_split(df_h2o, test_size=0.2, random_state=123)
#test , valid = train_test_split(test, test_size=0.5, random_state=123)
train = H2OFrame(train)
train['target'] = train['target'].asfactor()
test  = H2OFrame(test)
test['target'] = test['target'].asfactor()
#valid = H2OFrame(valid)
#valid['target'] = valid['target'].asfactor()

hy_pred = pd.DataFrame(h2o.as_list(best_model.predict(test)))['predict']
hy_test = h2o.as_list(test['target'])

H2OTr = best_model.accuracy(train=True, valid=False, xval=False)[0][1]
H2OTe = best_model.accuracy(train=False, valid=True, xval=False)[0][1]

best_model.accuracy(train=train, valid=test, xval=False)

#accuracy
m = [neigh, svc, LR, DT, BDT, BKNN, BGPC, BSVC, model]
label = ['KNeighborsClassifier','SupportVectorClassifier','LogisticRegression',
         'DecisionTreeClassifier',"BaggingDecisionTreeClassifier","BaggingKNeighborsClassifier",
         "BaggingGaussianProcessClassifier","BaggingSupportVectorClassifier",
         "ArtificialNeuralNetwork",type(best_model).__name__]
acc = pd.DataFrame({
    "model": label,#['KNN', "SVC", "LR",,"ANN","BagDT",'BagKNN','BagGPC','BagSVC',"H2ODRF",lb.head(rows=lb.nrows)[0,0].split('_')[0]+str('GLM')],
    "Train": [NTr,STr,LTr,DTr,BDTr,BKTr,BGTr,BSTr,np.mean(Tac),H2OTr],
    "Test" : [NTe,STe,LTe,DTe,BDTe,BKTe,BGTr,BSTe,np.mean(teac),H2OTe]
})

acc['avg'] = round((acc['Train'] + acc['Test'])/2, 6)
acc[acc["avg"] == acc["avg"].max()]
acc['BestModel'] = 0
for i in range(len(acc)):
  if acc['avg'][i] >= 90 and acc['avg'][i] < acc['avg'].max():
    acc.iloc[i,-1] = "good"
  elif acc['avg'][i] == acc['avg'].max():
    acc.iloc[i,-1] = "best"
  else:
    acc.iloc[i,-1] = "not good"
  
acc["Precision"] = np.zeros(len(acc))
acc["Recall"]    = np.zeros(len(acc))
acc["F1_Score"]  = np.zeros(len(acc))

acc

#type(BDT).__name__



#precision recall f1score
for i in range(len(m)): 

  if i == len(m)-1:
    acc.iloc[i,5]= np.mean(tepr)
    acc.iloc[i,6]= np.mean(tere)
    acc.iloc[i,7]= (2*acc.iloc[i,5]*acc.iloc[i,6])/(acc.iloc[i,5]+acc.iloc[i,6])
  
  else:
    #print(i)
    y_pred =  m[i].predict(X_test)
    p,r,f,_ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    acc.iloc[i,5]= p
    acc.iloc[i,6]= r
    acc.iloc[i,7]= f
  
p,r,f,_ = precision_recall_fscore_support(hy_test, hy_pred, average='macro')
acc.iloc[-1,5]= p
acc.iloc[-1,6]= r
acc.iloc[-1,7]= f


acc5	ANN	0.856098	0.872549	0.864323	0.880000	0.862745	0.871287

best_model.params

df  = pd.read_csv("/content/drive/MyDrive/heart.csv")

for i in range(len(df.columns)):
  if i != 0 and i != 3 and i != 4 and i != 7 and i != 9:
    df.iloc[:,i] = df.iloc[:,i].astype('category')
  


df.head()

#AUC ROC

df_td = df.copy()
X = df_td.iloc[:,0:-1].copy()
Y = df_td['target'].copy()
X_train, X_test, y_train, y_test = train_test_split(X, Y,
                                                    test_size=0.2, 
                                                    random_state=123)



plt.figure(1)
plt.rcParams["figure.figsize"] = [10, 5]
for i in range(len(m)+1):

  if i == len(m):
    y_pred = pd.DataFrame(h2o.as_list(best_model.predict(test)))
    y_test = h2o.as_list(test['target'])
    plt.figure(1)
    fpr, tpr, thresholds = roc_curve(y_test, y_pred["p1"])
    auc = metrics.roc_auc_score(y_test, y_pred["p1"])
    plt.plot(fpr, tpr,label=type(best_model).__name__ + '(area = {:.3f})'.format(auc))
    y_pred = 0;y_test=0

  else:
    if type(m[i]).__name__ == "Sequential":
      y_pred = m[i].predict(X_test).ravel()
    else:
      y_pred = m[i].predict_proba(X_test)[:, 1]


    fpr, tpr, thresholds = roc_curve(y_test, y_pred)
    auc = metrics.roc_auc_score(y_test,y_pred)
    plt.plot(fpr, tpr, label=str(label[i]) + '(area = {:.3f})'.format(auc))

plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('AUC ROC curve')
plt.legend(loc='best')
plt.show()

#best_model.explain(train)

#best_model.explain(test)



df_td = df.copy()
X = df_td.iloc[:,0:-1].copy()
Y = df_td['target'].copy()
X_train, X_test, y_train, y_test = train_test_split(X, Y,
                                                    test_size=0.2, 
                                                    random_state=123)

anndf = pd.DataFrame(
    {
    "ypred": np.zeros(len(y_testk)),
    "y_true": np.zeros(len(y_testk)),
    "y_pred": np.zeros(len(y_testk))
    })
anndf["ypred"] = m[8].predict(X_testk)
anndf["y_true"] = y_testk
for i in range(len(y_testk)):
  if anndf.iloc[i,0] >= 0.5:
    anndf.iloc[i,2] = 1

plt.rcParams["figure.figsize"] = [5, 4]
def CM(i):

  if i <8:
    y_true = y_test
    y_pred = m[i].predict(X_test)

  elif i==8 and type(m[i]).__name__ == 'Sequential':
    y_true = anndf['y_true']
    y_pred = anndf['y_pred']

  elif i==9:
    y_pred = pd.DataFrame(h2o.as_list(best_model.predict(test)))
    y_pred = y_pred["predict"]
    y_true = h2o.as_list(test['target'])
  

  cf_matrix=confusion_matrix(y_true, y_pred)
  group_names = ['True Negative',"False Positive","False Negative","True Positive"]
  group_counts = ["{0:0.0f}".format(value) for value in
                  cf_matrix.flatten()]
  group_percentages = ["{0:.2%}".format(value) for value in
                      cf_matrix.flatten()/np.sum(cf_matrix)]
  labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
            zip(group_names,group_counts,group_percentages)]
  labels = np.asarray(labels).reshape(2,2)
  title = "Confusion Matrix of " + label[i]
  sns.heatmap(cf_matrix, annot=labels, fmt="", cmap='Blues').set(
      title = title)
  plt.show()

for i in range(10):
  CM(i)

from joblib import dump, load

for i in range(9):

  file_name = label[i]+".joblib"
  dump(i, file_name)

m

